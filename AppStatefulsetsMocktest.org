# -*- ii: apisnoop; -*-
#+TITLE: StatefulSets Apps endpoints
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
#+PROPERTY: header-args:sql-mode :product postgres

* TODO Progress [2/5]                                                :export:
- [X] APISnoop org-flow : [[https://github.com/cncf/apisnoop/blob/master/tickets/k8s/][MyEndpoint.org]]
- [X] test approval issue : [[https://github.com/kubernetes/kubernetes/issues/][kubernetes/kubernetes#]]
- [ ] test pr : kuberenetes/kubernetes#
- [ ] two weeks soak start date : testgrid-link
- [ ] two weeks soak end date :
- [ ] test promotion pr : kubernetes/kubernetes#?
* Identifying an untested feature Using APISnoop                     :export:

According to this APIsnoop query, there are still some remaining RESOURCENAME endpoints which are untested.

with this query you can filter untested endpoints by their category and eligiblity for conformance.
e.g below shows a query to find all conformance eligible untested,stable,core endpoints

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      endpoint,
      -- k8s_action,
      -- path,
      -- description,
      kind
      FROM testing.untested_stable_endpoint
      where eligible is true
        and endpoint like '%StatefulSet%'
      --and category = 'core'
      order by kind, endpoint desc
      limit 25;
  #+end_src

  #+RESULTS: untested_stable_core_endpoints
  #+begin_SRC example
                    endpoint                   |    kind
  ---------------------------------------------+-------------
   patchAppsV1NamespacedStatefulSetScale       | Scale
   replaceAppsV1NamespacedStatefulSetStatus    | StatefulSet
   readAppsV1NamespacedStatefulSetStatus       | StatefulSet
   patchAppsV1NamespacedStatefulSetStatus      | StatefulSet
   patchAppsV1NamespacedStatefulSet            | StatefulSet
   listAppsV1StatefulSetForAllNamespaces       | StatefulSet
   deleteAppsV1CollectionNamespacedStatefulSet | StatefulSet
  (7 rows)

  #+end_SRC





* API Reference and feature documentation                            :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]
- [[https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/RESOURCENAME.go][client-go - RESOURCENAME]]

- [[https://kubernetes.io/docs/reference/kubectl/cheatsheet/#updating-resources][kubeclt scale / updating resources]]
- [[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#scale][Scale]]
- [[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#patch][kubectl-commands#patch]]
- [[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#replace][kubectl-commands#replace]]
- [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][StatefulSet]]
* The mock test                                                      :export:
** Test outline



*** 1. Create a Statefulset yaml file, namespace and Deployment


#+begin_src yaml :tangle statefulset_test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi

#+end_src
- Tangle to create the .yaml file - `,bt`


- See if the yaml file was created
#+begin_src shell :results raw
  pwd
 ls -al /home/riaan/Project/ticket-writing |grep .yaml

 #ls -al /home/ii/ticket-writing | grep yaml
#+end_src

#+RESULTS:
#+begin_example
/home/riaan/Project/ticket-writing
-rw-rw-r--  1 riaan riaan    404 Dec 11 06:39 deployment_test.yaml
-rw-rw-r--  1 riaan riaan   1029 Dec 11 06:39 statefulset_test.yaml
#+end_example





- Create a Namespace
#+begin_src shell :results raw
kubectl create namespace app-statefulset-tests
#+end_src

#+RESULTS:
#+begin_example
namespace/app-statefulset-tests created
#+end_example






- Create a Deployment
#+begin_src shell :results raw
kubectl apply -f statefulset_test.yaml --namespace=app-statefulset-tests
#+end_src

#+RESULTS:
#+begin_example
service/nginx created
statefulset.apps/web created
#+end_example






***  2. Find the statefulset
#+begin_src shell :results raw
  kubectl get statefulset -A | grep web
#+end_src

#+RESULTS:
#+begin_example
app-statefulset-tests   web    0/3     79m
#+end_example



WIP- Still to convert below this point

*** 3. Scale the Statefulset

#+begin_src shell :results raw
  kubectl scale statfulsets web -n app-statefulset-tests --replicas=4
  sleep 5
  kubectl get statefulset -A | grep web
  kubectl rollout status deployment.v1.apps/nginx-deployment -n app-deploy-tests
#+end_src

kubectl scale sts web --replicas=5


*** 4. Update the deployment:
#+begin_src shell :results raw
kubectl set image deployment.v1.apps/nginx-deployment -n app-deploy-tests nginx=nginx:1.16.0 --record
#+end_src




- and update it agaian, because we can...

#+begin_src shell :results raw
kubectl set image deployment.v1.apps/nginx-deployment -n app-deploy-tests nginx=nginx:1.16.1 --record
#+end_src




*** 5. Describe the deployments to see if it was updated with history
#+begin_src shell :results raw
  kubectl describe deployments -n app-deploy-tests | grep image
  kubectl rollout history deployment.v1.apps/nginx-deployment -n app-deploy-tests
#+end_src




*** 6. Describe the status of the deployment
#+begin_src shell :results raw
kubectl rollout status deployment.v1.apps/nginx-deployment -n app-deploy-tests
#+end_src




*** 7. The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress for a Deployment after 1 minute:

#+begin_src shell :results raw
#This command fail!
kubectl patch deployment.v1.apps/nginx-deployment -p -n app-deploy-tests '{"spec":{"progressDeadlineSeconds":60}}'
#+end_src





*** 8. Cleanup


- Delete the deployment and the namespace
#+begin_src shell :results raw
  kubectl delete statefulset web
  kubectl delete namespaces/app-statefulset-tests

#+end_src

#+RESULTS:
#+begin_example
namespace "app-statefulset-tests" deleted
#+end_example



- Look for deployment and namespace to check if it is deleted

#+begin_src shell :results raw
  kubectl get namespace app-deploy-tests
  kubectl get deployment nginx-deployment
#+end_src

- ALL DONE!



*** Delete audit events to check for success

- Count all audit events
#+begin_src sql-mode
select count(*) from testing.audit_event;
#+end_src

#+RESULTS:
#+begin_SRC example
 count
-------
  1503
(1 row)

#+end_SRC



- Delete all audit events
#+begin_src sql-mode
delete from testing.audit_event;
#+end_src

#+RESULTS:
#+begin_SRC example
DELETE 2228333
#+end_SRC




*** Test to see is new endpoint was hit by the test
#+begin_src sql-mode :eval never-export :exports both :session none
  select distinct  endpoint, useragent
                   -- to_char(to_timestamp(release_date::bigint), ' HH:MI') as time
  from testing.audit_event
  where endpoint ilike '%Deployment%'
    -- and release_date::BIGINT > round(((EXTRACT(EPOCH FROM NOW()))::numeric)*1000,0) - 60000
  and useragent like 'kubectl%'
  order by endpoint
  limit 100;

#+end_src

#+RESULTS:
#+begin_SRC example
 endpoint | useragent
----------+-----------
(0 rows)

#+end_SRC

*** About Scale enpoints

- The file [[https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/kubernetes/typed/apps/v1/deployment.go#L186-L228][deployment.go]] have three code sections that deal with scale endpoint replace-, read- and patchAppsV1NamespacedDeploymentScale.
  However neither of these tests blocks touch the endpoint

  The [[https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/statefulset.go#L848-L872][statefulsets.go]] file contain test similar scale endpoint for relace and read which make these endpoint conformance tested.

  If the statefulsets file could be used as a temple it could be applied to the deployment endpoint. The Patch --Deploymentscale endpoint was touch with a simple kubeclt command
  The same logic could then be applied to the Patch -- statefulsetsScale endpoint in another test.

**Patch**
 StatefulSet
 HTTP Request
 PATCH /apis/apps/v1/namespaces/{namespace}/statefulsets/{name}

 Deployment
 HTTP Request
 PATCH /apis/apps/v1/namespaces/{namespace}/deployments/{name}

 Patch for both statefulsets and deployments use the same HTTP Request logic


** Test the functionality in Go - AS IS IN statefulSet.go test
   #+NAME: Mock Test In Go
   #+begin_src go
             package main

             import (
               // "encoding/json"
               "fmt"
              // "context"
               "flag"
               "os"
              // v1 "k8s.io/api/core/v1"
               // "k8s.io/client-go/dynamic"
               // "k8s.io/apimachinery/pkg/runtime/schema"
               //metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
               "k8s.io/client-go/kubernetes"
               // "k8s.io/apimachinery/pkg/types"
               "k8s.io/client-go/tools/clientcmd"
                e2estatefulset "k8s.io/kubernetes/test/e2e/framework/statefulset"
          )

             func main() {
               // uses the current context in kubeconfig
               kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
               flag.Parse()
               config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
               if err != nil {
                   fmt.Println(err, "Could not build config from flags")
                   return
               }
               // make our work easier to find in the audit_event queries
               config.UserAgent = "live-test-writing"
               // creates the clientset
               ClientSet, _ := kubernetes.NewForConfig(config)
               // DynamicClientSet, _ := dynamic.NewForConfig(config)
               // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

               // TEST BEGINS HERE
                ssName := "ss"
                labels := map[string]string{
                 "foo": "bar",
                 "baz": "blah",
               headlessSvcName := "test"


              ss := e2estatefulset.NewStatefulSet(ssName, ns, headlessSvcName, 1, nil, nil, labels)
              setHTTPProbe(ss)
               ss, err := c.AppsV1().StatefulSets(ns).Create(context.TODO(), ss, metav1.CreateOptions{})
              ExpectNoError(err, "failed to create pod")
              e2estatefulset.WaitForRunningAndReady(c, *ss.Spec.Replicas, ss)
			        waitForStatus(c, ss)

              framework.ExpectEqual(*(ss.Spec.Replicas), int32(2))
                })
        })


               scale, err := c.AppsV1().StatefulSets(ns).GetScale(context.TODO(), ssName, metav1.GetOptions{})                                         
               if err != nil {                                                                                                                         
                       framework.Failf("Failed to get scale subresource: %v", err)
               }
               framework.ExpectEqual(scale.Spec.Replicas, int32(1))
               framework.ExpectEqual(scale.Status.Replicas, int32(1))

               scale.ResourceVersion = "" // indicate the scale update should be unconditional
               scale.Spec.Replicas = 2
               scaleResult, err := c.AppsV1().StatefulSets(ns).UpdateScale(context.TODO(), ssName, scale, metav1.UpdateOptions{})
               if err != nil {
                       framework.Failf("Failed to put scale subresource: %v", err)
               }
               framework.ExpectEqual(scaleResult.Spec.Replicas, int32(2))

               ss, err = c.AppsV1().StatefulSets(ns).Get(context.TODO(), ssName, metav1.GetOptions{})
               if err != nil {
                       framework.Failf("Failed to get statefulset resource: %v", err)
               }
  

		})
	})




      // helper function to inspect various interfaces
            func inspect(level int, name string, i interface{}) {
              fmt.Printf("Inspecting: %s\n", name)
              fmt.Printf("Inspect level: %d   Type: %T\n", level, i)
              switch level {
              case 1:
                 fmt.Printf("%+v\n\n", i)
              case 2:
                fmt.Printf("%#v\n\n", i)
              default:
                fmt.Printf("%v\n\n", i)
       }
     }


               // TEST ENDS HERE

               fmt.Println("[status] complete")

             }
   #+end_src

   #+RESULTS: Mock Test In Go
   #+begin_src go
   #+end_src









** Test the functionality in Go - As updated by Riaankl accoding to rc.go example for Patch ---Scale
   #+NAME: Mock Test In Go
   #+begin_src go
             package main

             import (
               "encoding/json"
               "fmt"
              // "context"
               "flag"
               "os"
              // v1 "k8s.io/api/core/v1"
               "k8s.io/client-go/dynamic"
               // "k8s.io/apimachinery/pkg/runtime/schema"
               //metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
               "k8s.io/client-go/kubernetes"
               // "k8s.io/apimachinery/pkg/types"
               "k8s.io/client-go/tools/clientcmd"
                e2estatefulset "k8s.io/kubernetes/test/e2e/framework/statefulset"
          )

             func main() {
               // uses the current context in kubeconfig
               kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
               flag.Parse()
               config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
               if err != nil {
                   fmt.Println(err, "Could not build config from flags")
                   return
               }
               // make our work easier to find in the audit_event queries
               config.UserAgent = "live-test-writing"
               // creates the clientset
               ClientSet, _ := kubernetes.NewForConfig(config)
               // DynamicClientSet, _ := dynamic.NewForConfig(config)
               // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

               // TEST BEGINS HERE
                ssName := "ss"
                labels := map[string]string{
                 "foo": "bar",
                 "baz": "blah",
               headlessSvcName := "test"


                       ss := e2estatefulset.NewStatefulSet(ssName, ns, headlessSvcName, 1, nil, nil, labels)
                       setHTTPProbe(ss)
                       ss, err := c.AppsV1().StatefulSets(ns).Create(context.TODO(), ss, metav1.CreateOptions{})
                       framework.ExpectNoError(err)
                       e2estatefulset.WaitForRunningAndReady(c, *ss.Spec.Replicas, ss)
                       waitForStatus(c, ss)

                       scale, err := c.AppsV1().StatefulSets(ns).GetScale(context.TODO(), ssName, metav1.GetOptions{})                                                                             
                       framework.Logf("scale: %#v", scale)                                                                                                                                         
                       framework.Logf("err: %+v", err)                                                                                                                                             
                       if err != nil {                                                                                                                                                             
                               framework.Failf("Failed to get scale subresource: %v", err)
                       }
                       framework.ExpectEqual(scale.Spec.Replicas, int32(1))
                       framework.ExpectEqual(scale.Status.Replicas, int32(1))
                       ginkgo.By("updating a scale subresource")
                       scale.ResourceVersion = "" // indicate the scale update should be unconditional
                       scale.Spec.Replicas = 2
                       ssScalePatchPayload, err := json.Marshal(autoscalingv1.Scale{
                               Spec: autoscalingv1.ScaleSpec{
                                       Replicas: scale.Spec.Replicas,
                               },
                       })
                       scaleResult, err := c.AppsV1().StatefulSets(ns).Patch (context.TODO(), ssName, types.StrategicMergePatchType, []byte(ssScalePatchPayload), metav1.PatchOptions{}, "scale")
                       framework.Logf("scaleResult: %#v", scaleResult)
                       framework.Logf("err: %#v", err)
                       x := scaleResult.Status.ReadyReplicas
                       framework.Logf("ReadyReplicas: %#v", x)
                       if err != nil {
                               framework.Failf("Failed to put scale subresource: %v", err)
                       }
                       framework.ExpectEqual(scaleResult.Spec.Replicas, int32(2))

                       ss, err = c.AppsV1().StatefulSets(ns).Get(context.TODO(), ssName, metav1.GetOptions{})                                                                                      
                       if err != nil {                                                                                                                                                             
                               framework.Failf("Failed to get statefulset resource: %v", err)                                                                                                      
                       }                                                                                                                                                                           
                       framework.ExpectEqual(*(ss.Spec.Replicas), int32(0))                                                                                                                        
               })                                                                                                                                                                                  
       })                                                                                                                                                                                          
                                                                                                                                                                                                   
 














      // helper function to inspect various interfaces
            func inspect(level int, name string, i interface{}) {
              fmt.Printf("Inspecting: %s\n", name)
              fmt.Printf("Inspect level: %d   Type: %T\n", level, i)
              switch level {
              case 1:
                 fmt.Printf("%+v\n\n", i)
              case 2:
                fmt.Printf("%#v\n\n", i)
              default:
                fmt.Printf("%v\n\n", i)
       }
     }


               // TEST ENDS HERE

               fmt.Println("[status] complete")

             }
   #+end_src

   #+RESULTS: Mock Test In Go
   #+begin_src go
   #+end_src






* Verifying increase in coverage with APISnoop                       :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent
      from testing.audit_event
      where useragent like 'live%';
  #+end_src

  #+RESULTS:
  :  useragent
  : -----------
  : (0 rows)
  :

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from testing.endpoint_hit_by_new_test;
#+end_src

#+RESULTS:
#+begin_SRC example
 useragent | endpoint | hit_by_ete | hit_by_new_test
-----------+----------+------------+-----------------
(0 rows)

#+end_SRC

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from testing.projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             862 |          343 |          343 |                0
  (1 row)

  #+end_SRC




#+begin_src sql-mode :exports both :session none
  select distinct  endpoint, right(useragent,73) AS useragent
  from testing.audit_event
   where useragent ilike '%scale subresource2%'
   -- where endpoint ilike '%AppsV1NamespacedStatefulSet%'
   and release_date::BIGINT > round(((EXTRACT(EPOCH FROM NOW()))::numeric)*1000,0) - 60000
  and useragent like 'e2e%'
  order by endpoint
  limit 30;

#+end_src

#+RESULTS:
#+begin_SRC example
                 endpoint                  |                                 useragent
-------------------------------------------+---------------------------------------------------------------------------
 connectCoreV1GetNodeProxyWithPath         | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 createAppsV1NamespacedStatefulSet         | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 createCoreV1Namespace                     | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 createCoreV1NamespacedPod                 | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 createCoreV1NamespacedService             | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 deleteAppsV1NamespacedStatefulSet         | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 deleteCoreV1Namespace                     | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listAppsV1NamespacedStatefulSet           | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1NamespacedEvent                 | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1NamespacedPersistentVolumeClaim | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1NamespacedPod                   | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1NamespacedServiceAccount        | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1Node                            | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listCoreV1PersistentVolume                | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 listPolicyV1beta1PodSecurityPolicy        | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 patchAppsV1NamespacedStatefulSetScale     | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 readAppsV1NamespacedStatefulSet           | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 readAppsV1NamespacedStatefulSetScale      | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 readCoreV1Node                            | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
 replaceAppsV1NamespacedStatefulSet        | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
                                           | [StatefulSetBasic] should have a working scale subresource2 [Conformance]
(21 rows)

#+end_SRC










* Convert to Ginkgo Test
** Ginkgo Test
  :PROPERTIES:
  :ID:       gt001z4ch1sc00l
  :END:
* Final notes                                                        :export:
If a test with these calls gets merged, **test coverage will go up by N points**

This test is also created with the goal of conformance promotion.

-----
/sig testing

/sig architecture

/area conformance


* scratch
#+BEGIN_SRC
CREATE OR REPLACE VIEW "public"."untested_stable_endpoints" AS
  SELECT
    ec.*,
    ao.description,
    ao.http_method
    FROM endpoint_coverage ec
           JOIN
           api_operation_material ao ON (ec.bucket = ao.bucket AND ec.job = ao.job AND ec.operation_id = ao.operation_id)
   WHERE ec.level = 'stable'
     AND tested is false
     AND ao.deprecated IS false
     AND ec.job != 'live'
   ORDER BY hit desc
            ;
#+END_SRC

#+begin_src sql-mode :exports both :session none
  select distinct  endpoint, left(useragent,93) AS useragent
  -- select distinct  endpoint, right(useragent,73) AS useragent
  from testing.audit_event
   where useragent ilike '%kubectl%'
   -- where endpoint ilike '%AppsV1NamespacedStatefulSet%'
   -- and release_date::BIGINT > round(((EXTRACT(EPOCH FROM NOW()))::numeric)*1000,0) - 60000
   -- and useragent like 'e2e%'
  order by endpoint
  limit 10;

#+end_src

#+RESULTS:
#+begin_SRC example
                  endpoint                   |                    useragent
---------------------------------------------+--------------------------------------------------
 createCoreV1Namespace                       | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getAdmissionregistrationV1APIResources      | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getAdmissionregistrationV1beta1APIResources | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getApiextensionsV1APIResources              | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getApiextensionsV1beta1APIResources         | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getApiregistrationV1APIResources            | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getApiregistrationV1beta1APIResources       | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getAPIVersions                              | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getAppsV1APIResources                       | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
 getAuthenticationV1APIResources             | kubectl/v1.19.0 (linux/amd64) kubernetes/e199641
(10 rows)

#+end_SRC
