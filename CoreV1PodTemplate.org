#+TITLE: Test Writing Flow
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
* Filling the Gaps in Kubernetes Test Coverage

Are the Kubernetes behaviors your applications actually require well tested and guaranteed to be available on all cloud providers?

In this session, you will learn how to ensure your Kubernetes API surface area usage is exercised by tests all Kubernetes Certified Service Providers must pass.

We will cover:
- the e2e test suite
- automation that runs the suite before code is merged into Kubernetes.
- the API surface area covered by these tests
- the API surface area required by several popular applications.
- Identifying the untested API surface area your applications require
- Contributing tests that increase API surface coverage
- Promoting tests to Conformance

* TODO [91%] Cluster Setup
  :PROPERTIES:
  :LOGGING:  nil
  :END:
** DONE Connect demo to right eye

   #+begin_src tmate :session foo:hello :eval never-export
     echo "What parts of Kubernetes do you depend on $USER?"
   #+end_src

** DONE Create a K8s cluster using KIND

[[file:~/cncf/apisnoop/deployment/k8s/kind-cluster-config.yaml::#%20kind-cluster-config.yaml][kind-cluster-config.yaml (enabling Dynamic Audit Logging)]]

   #+BEGIN_SRC tmate :eval never-export :session foo:cluster :prologue "cd ~/cncf/apisnoop/\n"
     # Uncomment the next line if you want to clean up a previously created cluster.
     kind delete cluster
     kind create cluster --config ~/cncf/apisnoop/deployment/k8s/kind-cluster-config.yaml
   #+END_SRC
   
** DONE Grab cluster info, to ensure it is up.
   
   #+BEGIN_SRC shell :results replace 
     kubectl cluster-info
   #+END_SRC

   #+RESULTS:
   #+begin_EXAMPLE
   Kubernetes master is running at https://127.0.0.1:37015
   KubeDNS is running at https://127.0.0.1:37015/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
   #+end_EXAMPLE

** DONE Prepull our images
*** apisnoop
   #+BEGIN_SRC tmate :eval never-export :session x:img :prologue "cd ~/cncf/apisnoop/\n"
     # Run twice... first time will pull and save, second will load into kind
     kind load image-archive   hasura:2019-12-03-16-31.docker-image \
       || docker pull raiinbow/hasura:2019-12-03-16-31 \
       && docker save raiinbow/hasura:2019-12-03-16-31 -o hasura:2019-12-03-16-31.docker-image

     kind load image-archive   postgres:2019-12-03-14-19.docker-image \
       || docker pull raiinbow/postgres:2019-12-03-14-19 \
       && docker save raiinbow/postgres:2019-12-03-14-19 -o postgres:2019-12-03-14-19.docker-image

     kind load image-archive   auditlogger:2019-12-08-31.docker-image \
       || docker pull raiinbow/auditlogger:2019-12-08-31 \
       && docker save raiinbow/auditlogger:2019-12-08-31 -o auditlogger:2019-12-08-31.docker-image

   #+END_SRC
*** check
   #+begin_src shell :eval never-export :exports both
     #ps ax | grep kind\ load | grep -v grep
     docker exec kind-control-plane crictl img
   #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  IMAGE                                TAG                 IMAGE ID            SIZE
  docker.io/kindest/kindnetd           0.5.3               aa67fec7d7ef7       80.3MB
  docker.io/raiinbow/hasura            2019-12-03-16-31    a6d3afa615805       53.1MB
  docker.io/raiinbow/postgres          2019-12-03-14-19    e712ce7cc2a67       1.2GB
  k8s.gcr.io/coredns                   1.6.2               bf261d1579144       44.2MB
  k8s.gcr.io/etcd                      3.3.15-0            b2756210eeabf       248MB
  k8s.gcr.io/kube-apiserver            v1.16.3             392249bd86967       185MB
  k8s.gcr.io/kube-controller-manager   v1.16.3             808025b3748ef       128MB
  k8s.gcr.io/kube-proxy                v1.16.3             f4fd1d7052b4e       103MB
  k8s.gcr.io/kube-scheduler            v1.16.3             1974a03197540       105MB
  k8s.gcr.io/pause                     3.1                 da86e6ba6ca19       746kB
  #+end_EXAMPLE

** DONE Check on cluster
#+begin_src shell :eval never-export :exports both
docker exec kind-control-plane crictl ps 
#+end_src

#+RESULTS:
#+begin_EXAMPLE
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
adae42c6657bf       bf261d1579144       8 seconds ago       Running             coredns                   0                   45ade81123bb6
f31afbabe238b       bf261d1579144       8 seconds ago       Running             coredns                   0                   b9a4a19714e98
ec7d93d0d7ba0       f4fd1d7052b4e       21 seconds ago      Running             kube-proxy                0                   1e6ff85f22919
38f9df4ecbd3e       aa67fec7d7ef7       21 seconds ago      Running             kindnet-cni               0                   abf00a446c383
fe56ba6568ec5       b2756210eeabf       45 seconds ago      Running             etcd                      0                   efa35b9112c9a
9ddc803fe7a52       392249bd86967       45 seconds ago      Running             kube-apiserver            0                   de7f037aa462f
49b8133b4d89d       808025b3748ef       45 seconds ago      Running             kube-controller-manager   0                   d879349cd7a00
d8ca66c768b8c       1974a03197540       45 seconds ago      Running             kube-scheduler            0                   314cde7e9acc5
#+end_EXAMPLE

** DONE Deploy APISnoop                                              :export:

   #+begin_src shell :exports both :eval never-export :wrap "SRC text"
     kubectl apply -f ~/cncf/apisnoop/deployment/k8s/raiinbow.yaml
   #+end_src

** DONE Verify Pods Running
   #+begin_src shell
     kubectl get pods --all-namespaces
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE
   default       apisnoop-auditlogger-6458f9545-hgrgf         1/1     Running   0          70s
   default       hasura-5d447cc65d-w42n5                      1/1     Running   0          70s
   default       postgres-7b494768d5-2h28g                    1/1     Running   0          70s
   kube-system   coredns-5644d7b6d9-hq5js                     1/1     Running   0          114s
   kube-system   coredns-5644d7b6d9-qr4bc                     1/1     Running   0          114s
   kube-system   etcd-kind-control-plane                      1/1     Running   0          45s
   kube-system   kindnet-nrhjz                                1/1     Running   0          114s
   kube-system   kube-apiserver-kind-control-plane            1/1     Running   0          67s
   kube-system   kube-controller-manager-kind-control-plane   1/1     Running   0          46s
   kube-system   kube-proxy-297gd                             1/1     Running   0          114s
   kube-system   kube-scheduler-kind-control-plane            1/1     Running   0          46s
   #+end_EXAMPLE
** DONE Setup Port-Forwarding from us to sharing to the cluster

   We'll setup port-forwarding for postgres, to let us easily send queries from within our org file.
   You can check the status of the port-forward in your right eye.
   #+BEGIN_SRC tmate :eval never-export :session foo:postgres
     export GOOGLE_APPLICATION_CREDENTIALS=$HOME/.gcreds.json
     # export K8S_NAMESPACE="kube-system"
     # kubectl config set-context $(kubectl config current-context) --namespace=$K8S_NAMESPACE 2>&1 > /dev/null
     POSTGRES_POD=$(kubectl get pod --selector=io.apisnoop.db=postgres -o name | sed s:pod/::)
     POSTGRES_PORT=$(kubectl get pod $POSTGRES_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
     kubectl port-forward $POSTGRES_POD $(id -u)1:$POSTGRES_PORT
   #+END_SRC

   Then we'll setup a port-forward for hasura, so our web app can query it directly.
   #+BEGIN_SRC tmate :eval never-export :session foo:hasura
     HASURA_POD=$(kubectl get pod --selector=io.apisnoop.graphql=hasura -o name | sed s:pod/::)
     HASURA_PORT=$(kubectl get pod $HASURA_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
     kubectl port-forward $HASURA_POD --address 0.0.0.0 8080:$HASURA_PORT
   #+END_SRC
** DONE Connect Org to our apisnoop db
   #+NAME: ReConnect org to postgres
   #+BEGIN_SRC emacs-lisp :results silent
     (if (get-buffer "*SQL: postgres:data*")
         (with-current-buffer "*SQL: postgres:data*"
           (kill-buffer)))
     (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
   #+END_SRC
** DONE Check it all worked

   Once the postgres pod has been up for at least three minutes, you can check if it all works.

   Running ~\d+~ will list all the tables and views in your db, and their size.
   First,you want to ensure that relations _are_ found.  IF not, something happened with postgres and you should check the logs (check out [[#footnotes]] for more info.)

   There should be about a dozen views, and two tables.  The table ~bucket_job_swagger~ should be about 3712kb.  The table ~raw_audit_event~ should be about 416mb.  If either show as 8192 bytes, it means no data loaded.  Check the Hasura logs in this case, to see if there was an issue with the migration.

   #+begin_src sql-mode :results replace
     \d+
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                                                              List of relations
    Schema |               Name               |       Type        |  Owner   |  Size   |                                    Description                                    
   --------+----------------------------------+-------------------+----------+---------+-----------------------------------------------------------------------------------
    public | api_operation_material           | materialized view | apisnoop | 3688 kB | details on each operation_id as taken from the openAPI spec
    public | api_operation_parameter_material | materialized view | apisnoop | 6016 kB | the parameters for each operation_id in open API spec
    public | audit_event                      | view              | apisnoop | 0 bytes | a record for each audit event in an audit log
    public | bucket_job_swagger               | table             | apisnoop | 3712 kB | metadata for audit events  and their respective swagger.json
    public | endpoint_coverage                | view              | apisnoop | 0 bytes | the test hits and conformance test hits per operation_id & other useful details
    public | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | list endpoints hit during our live auditing alongside their current test coverage
    public | projected_change_in_coverage     | view              | apisnoop | 0 bytes | overview of coverage stats if the e2e suite included your tests
    public | raw_audit_event                  | table             | apisnoop | 394 MB  | a record for each audit event in an audit log
    public | stable_endpoint_stats            | view              | apisnoop | 0 bytes | coverage stats for entire test run, looking only at its stable endpoints
    public | untested_stable_core_endpoints   | view              | apisnoop | 0 bytes | list stable core endpoints not hit by any tests, according to their test run
   (10 rows)

   #+end_src

** DONE Check current coverage
   It can be useful to see the current level of testing according to your baseline audit log (by default the last successful test run on master).

   You can view this with the query:
   #+NAME: stable endpoint stats
   #+begin_src sql-mode
     select * from stable_endpoint_stats where job != 'live';
   #+end_src

   #+RESULTS: stable endpoint stats
   #+begin_src sql-mode
            job         |    date    | total_endpoints | test_hits | conf_hits | percent_tested | percent_conf_tested 
   ---------------------+------------+-----------------+-----------+-----------+----------------+---------------------
    1203778996630720516 | 2019-12-08 |             438 |       183 |       129 |          41.78 |               29.45
   (1 row)

   #+end_src

** TODO Stand up, Stretch, and get a glass of water
   You did it! By hydration and pauses are important.  Take some you time, and drink a full glass of water!
* Identify an untested feature Using APISnoop                        :export:

We can query a few tables, this is a simple core api query for get APIs that do not hit volumes.

  #+begin_src sql-mode :eval never-export :exports both
    SELECT
      operation_id,
      k8s_action,
      path,
      description
      FROM untested_stable_core_endpoints
      where path not like '%volume%'
      and operation_id like '%PodTemplate%'
     ORDER BY operation_id desc
     LIMIT 25
           ;
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
                  operation_id                 |    k8s_action    |                        path                        |                description                 
  ---------------------------------------------+------------------+----------------------------------------------------+--------------------------------------------
   replaceCoreV1NamespacedPodTemplate          | put              | /api/v1/namespaces/{namespace}/podtemplates/{name} | replace the specified PodTemplate
   readCoreV1NamespacedPodTemplate             | get              | /api/v1/namespaces/{namespace}/podtemplates/{name} | read the specified PodTemplate
   patchCoreV1NamespacedPodTemplate            | patch            | /api/v1/namespaces/{namespace}/podtemplates/{name} | partially update the specified PodTemplate
   listCoreV1PodTemplateForAllNamespaces       | list             | /api/v1/podtemplates                               | list or watch objects of kind PodTemplate
   deleteCoreV1CollectionNamespacedPodTemplate | deletecollection | /api/v1/namespaces/{namespace}/podtemplates        | delete collection of PodTemplate
  (5 rows)

  #+end_src

  You can iterate over a query until you have a set of endpoints you'd like to write a test for, usually by adjusting the columns you view or by extending the where clause to filter more specifically.
* Use API Reference to Lightly Document the Feature                  :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
Finding documentation for this API
* The mock test                                                   :export:
  This is where the test code goes. It is useful to seperate it into blocks which can be evaluted independently.

  You can write tests in a variety of languages, outlined in [[https://kubernetes.io/docs/reference/using-api/client-libraries/]["Client Libraries"]] on k8s reference page.

  Whichever language you choose, you want to make sure to set the useragent to something starting with ~live-test~.  This ensures apisnoop's queries around testing work correctly.

  We've included sample tests in Go and Javascript.

** Example in Go

   #+begin_src shell
     go get -v -u k8s.io/apimachinery/pkg/apis/meta/v1
     go get -v -u k8s.io/client-go/kubernetes
     go get -v -u k8s.io/client-go/tools/clientcmd
   #+end_src

   #+begin_src go
     package main

     import (
       "fmt"
       "flag"
       "os"
       //"encoding/json"
       v1 "k8s.io/api/core/v1"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       "k8s.io/client-go/tools/clientcmd"
       "k8s.io/apimachinery/pkg/types"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
          flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
         fmt.Println(err)
         return
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       clientset, _ := kubernetes.NewForConfig(config)

       podTemplateName := "nginx-pod-template"

       // get a list of PodTemplates
       podTemplateList, err := clientset.CoreV1().PodTemplates("").List(metav1.ListOptions{})
       if err != nil {
         fmt.Println("[error]", err)
         return
       }
       if len(podTemplateList.Items) > 0 {
         fmt.Println("[error] templates should not be populated")
         return
       }

       fmt.Println("[status] no PodTemplates found")

       // create a PodTemplate
       _, err = clientset.CoreV1().PodTemplates("default").Create(&v1.PodTemplate{
         ObjectMeta: metav1.ObjectMeta{
           Name: podTemplateName,
         },
         Template: v1.PodTemplateSpec{
           Spec: v1.PodSpec{
             Containers: []v1.Container{
               {Name: "nginx", Image: "nginx"},
             },
           },
         },
       })
       if err != nil {
         fmt.Println("[error]", err)
         return
       }

       fmt.Println("[status] PodTemplate created")

       // get template
       podTemplateRead, err := clientset.CoreV1().PodTemplates("default").Get(podTemplateName, metav1.GetOptions{})
       if err != nil {
         fmt.Println("[error]", err)
         return
       }
       if podTemplateRead.ObjectMeta.Name != podTemplateName {
         fmt.Println("[error] PodTemplate name doesn't match")
         return
       }

       fmt.Println("[status] found created PodTemplate")

       // patch template
       PodTemplatePatch := fmt.Sprintf(`{"metadata":{"labels":{"a":"1"}}}`)
       _, err = clientset.CoreV1().PodTemplates("default").Patch(podTemplateName, types.StrategicMergePatchType, []byte(PodTemplatePatch))
       if err != nil {
         fmt.Println("[error]", err)
         return
       }

       fmt.Println("[status] patched PodTemplate with label a=1")

       // get template (ensure label is there)
       podTemplateRead, err = clientset.CoreV1().PodTemplates("default").Get(podTemplateName, metav1.GetOptions{})
       if err != nil {
         fmt.Println("[error]", err)
         return
       }
       if podTemplateRead.ObjectMeta.Labels["a"] != "1" {
         fmt.Println("[error] template doesn't contain the label a=1")
         return
       }

       fmt.Println("[status] found label on PodTemplate")

       // list PodTemplates on all namespaces by label a=1
       podTemplateListWithLabel, err := clientset.CoreV1().PodTemplates("").List(metav1.ListOptions{
         LabelSelector: "a=1",
       })
       if err != nil {
         fmt.Println("[error]", err)
         return
       }
       foundPodTemplateWithLabel := false
       for _, podTemplate := range podTemplateListWithLabel.Items {
         if podTemplate.ObjectMeta.Name == podTemplateName && podTemplate.ObjectMeta.Labels["a"] == "1" {
           foundPodTemplateWithLabel = true
         }
       }
       if foundPodTemplateWithLabel == false || len(podTemplateListWithLabel.Items) == 0 {
         fmt.Println("[error] PodTemplate doesn't contain the label")
         return
       }

       fmt.Println("[status] found PodTemplate by label")

       err = clientset.CoreV1().PodTemplates("default").Delete(podTemplateName, &metav1.DeleteOptions{})
       if err != nil {
         fmt.Println("[error]", err)
         return
       }

       fmt.Println("[status] deleted PodTemplate")

       podTemplateListWithLabel, err = clientset.CoreV1().PodTemplates("").List(metav1.ListOptions{
         LabelSelector: "a=1",
       })
       if err != nil {
         fmt.Println("[error]", err)
         return
       }
       if len(podTemplateListWithLabel.Items) > 0 {
         fmt.Println("[error] list returned a PodTemplate matching the requested labels")
         return
       }

       fmt.Println("[status] no PodTemplates found")

       fmt.Println("[status] complete")
     }
   #+end_src

   #+RESULTS:
   : [status] no PodTemplates found
   : [status] PodTemplate created
   : [status] found created PodTemplate
   : [status] patched PodTemplate with label a=1
   : [status] found label on PodTemplate
   : [status] found PodTemplate by label
   : [status] deleted PodTemplate
   : [status] no PodTemplates found
   : [status] complete

* Verify with APISnoop                                               :export:

  #+begin_src sql-mode :eval never-export :exports both
    select distinct useragent from audit_event where bucket='apisnoop' and useragent not like 'kube%' and useragent not like 'coredns%' and useragent not like 'kindnetd%';
    -- select * from endpoints_hit_by_new_test where useragent like 'Swagger%' or useragent like 'live-%';
    --select * from endpoints_hit_by_new_test where useragent like 'Swagger%';
    -- select * from endpoints_hit_by_new_test where useragent like 'live%';
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
       useragent     
  -------------------
   live-test-writing
  (1 row)

  #+end_src

  NOTE: for the projected change in coverage, your test functions must be configured with a useragent that starts with ~live-test~, otherwise endpoints hit by that test won't be counted as part of new coverage.
 
  #+begin_src sql-mode
  select * from endpoints_hit_by_new_test where useragent like 'live%';
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
       useragent     |             operation_id              | hit_by_ete | hit_by_new_test 
  -------------------+---------------------------------------+------------+-----------------
   live-test-writing | createCoreV1NamespacedPodTemplate     |        842 |               2
   live-test-writing | deleteCoreV1NamespacedPodTemplate     |          2 |               2
   live-test-writing | listCoreV1PodTemplateForAllNamespaces |          0 |               3
   live-test-writing | patchCoreV1NamespacedPodTemplate      |          0 |               2
   live-test-writing | readCoreV1NamespacedPodTemplate       |          0 |               2
  (5 rows)

  #+end_src
 
  #+begin_src sql-mode :eval never-export :exports both
    select * from projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
     category    | total_endpoints | old_coverage | new_coverage | change_in_number 
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             438 |          183 |          186 |                3
  (1 row)

  #+end_src

* Open Tasks
  Set any open tasks here, using org-todo
** DONE Live Your Best Life
** Comments
   #+begin_src sql-mode
COMMENT ON TABLE bucket_job_swagger IS 'raw data taken from audit events relevant swagger.json';
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
   COMMENT
   #+end_src
   
   #+begin_src sql-mode
   \d+ 
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                                                 List of relations
    Schema |               Name               |       Type        |  Owner   |  Size   |                      Description                       
   --------+----------------------------------+-------------------+----------+---------+--------------------------------------------------------
    public | api_operation_material           | materialized view | apisnoop | 3688 kB | 
    public | api_operation_parameter_material | materialized view | apisnoop | 6016 kB | 
    public | audit_event                      | view              | apisnoop | 0 bytes | 
    public | bucket_job_swagger               | table             | apisnoop | 3712 kB | raw data taken from audit events relevant swagger.json
    public | change_in_coverage               | view              | apisnoop | 0 bytes | 
    public | change_in_tests                  | view              | apisnoop | 0 bytes | 
    public | endpoint_coverage                | view              | apisnoop | 0 bytes | 
    public | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | 
    public | projected_change_in_coverage     | view              | apisnoop | 0 bytes | 
    public | raw_audit_event                  | table             | apisnoop | 407 MB  | 
    public | stable_endpoint_stats            | view              | apisnoop | 0 bytes | 
    public | untested_stable_core_endpoints   | view              | apisnoop | 0 bytes | 
   (12 rows)

   #+end_src

* Footnotes :neverexport:
  :PROPERTIES:
  :CUSTOM_ID: footnotes
  :END:
** Load Logs to Help Debug Cluster
   #:PROPERTIES:
   #:header-args:tmate+: :prologue (concat "cd " (file-name-directory buffer-file-name) "../../apisnoop/apps\n. .loadenv\n")
   #:END:
*** hasura logs

    #+BEGIN_SRC tmate :eval never-export :session foo:hasura_logs
      HASURA_POD=$(\
                   kubectl get pod --selector=io.apisnoop.graphql=hasura -o name \
                       | sed s:pod/::)
      kubectl logs $HASURA_POD -f
    #+END_SRC

*** postgres logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      POSTGRES_POD=$(\
                     kubectl get pod --selector=io.apisnoop.db=postgres -o name \
                         | sed s:pod/::)
      kubectl logs $POSTGRES_POD -f
    #+END_SRC

*** auditlogger logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      AUDITLOGGER_POD=$(\
                     kubectl get pod --selector=app=apisnoop-auditlogger -o name \
                         | sed s:pod/::)
      kubectl logs $AUDITLOGGER_POD -f
    #+END_SRC

** Manually load swagger or audit events
   If you ran through the full setup, but were getting 0's in the stable_endpint_stats, it means the table migrations were successful, but no data was loaded.

   You can verify data loaded with the below query.  ~bucket_job_swagger~ should have a size around 3600kb and raw_audit_event should have a size around 412mb.

   #+NAME: Verify Data Loaded
   #+begin_src sql-mode
     \dt+
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
     List of relations
       Schema |        Name        | Type  |  Owner   |  Size   | Description
       --------+--------------------+-------+----------+---------+-------------
       public | bucket_job_swagger | table | apisnoop | 3600 kB |
       public | raw_audit_event    | table | apisnoop | 412 MB  |
       (2 rows)

   #+end_src

   If either shows a size of ~8192 bytes~, you'll want to manually load it, refresh materialized views, then check again.

   if you want to load a particular bucket or job, you can name them as the first and second argument of these functions.
   e.g
   : select * from load)swagger('ci-kubernetes-beta', 1122334344);
   will load that specific bucket/job combo.
   : select * from load_swagger('ci-kubernetes-beta');
   will load the latest successful test run for ~ci-kubernetes-beta~
   : select * from load_swagger('ci-kubernetes-beta', null, true);
   will load the latest successful test run for ~ci-kubernetes-beta~, but with bucket and job set to 'apisnoop/live' (used for testing).
   #+NAME: Manually load swaggers
   #+begin_src sql-mode
     select * from load_swagger();
     select * from load_swagger(null, null, true);
   #+end_src

   #+NAME: Manually load audit events
   #+begin_src sql-mode
     select * from load_audit_events();
   #+end_src

   #+NAME: Refresh Materialized Views
   #+begin_src sql-mode
     REFRESH MATERIALIZED VIEW api_operation_material;
     REFRESH MATERIALIZED VIEW api_operation_parameter_material;
   #+end_src
** 200: stuff
*** 250: api_schema view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_view_api_schema.up.sql
    :END:
**** Create

  #+NAME: api_schema view
  #+BEGIN_SRC sql-mode 
    CREATE OR REPLACE VIEW "public"."api_schema" AS 
     SELECT 
        bjs.bucket,
        bjs.job,
        d.key AS schema_name,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
        (d.value ->> 'type'::text) AS resource_type,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
        ARRAY(SELECT jsonb_array_elements_text(d.value -> 'required')) as required_fields,
        (d.value -> 'properties'::text) AS properties,
        d.value
       FROM bucket_job_swagger bjs
         , jsonb_each((bjs.swagger -> 'definitions'::text)) d(key, value)
       GROUP BY bjs.bucket, bjs.job, d.key, d.value;

  #+END_SRC

  #+RESULTS: api_schema view
  #+begin_src sql-mode
  CREATE VIEW
  #+end_src

*** 260: api_schema_field view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/260_view_api_schema_field.up.sql
    :END:
**** Create
 #+NAME: api_schema_field view
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_schema_field" AS 
     SELECT api_schema.schema_name as field_schema,
            d.key AS field_name,
            replace(
              CASE
              WHEN d.value->>'type' = 'string' THEN 'string'
              WHEN d.value->>'type' IS NULL THEN d.value->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->> 'type' IS NULL
                THEN d.value->'items'->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->>'$ref' IS NULL
                THEN d.value->'items'->>'type'
              ELSE 'integer'::text
              END, '#/definitions/','') AS field_kind,
            CASE
            WHEN d.value->>'type' IS NULL THEN 'subtype'
            ELSE d.value->>'type'
              END AS field_type,
            d.value->>'description' AS description,
            CASE
            WHEN d.key = ANY(api_schema.required_fields) THEN true
            ELSE false
              END AS required,
            CASE
            WHEN (   d.value->>'description' ilike '%This field is alpha-level%'
                  or d.value->>'description' ilike '%This is an alpha field%'
                  or d.value->>'description' ilike '%This is an alpha feature%') THEN 'alpha'
            WHEN (   d.value->>'description' ilike '%This field is beta-level%'
                  or d.value->>'description' ilike '%This field is beta%'
                  or d.value->>'description' ilike '%This is a beta feature%'
                  or d.value->>'description' ilike '%This is an beta feature%'
                  or d.value->>'description' ilike '%This is an beta field%') THEN 'beta'
            ELSE 'ga'
              END AS release,
            CASE
            WHEN  d.value->>'description' ilike '%deprecated%' THEN true
             ELSE false
             END AS deprecated,
            CASE
            WHEN ( d.value->>'description' ilike '%requires the % feature gate to be enabled%'
                  or d.value->>'description' ilike '%depends on the % feature gate being enabled%'
                  or d.value->>'description' ilike '%requires the % feature flag to be enabled%'
                  or d.value->>'description' ilike '%honored if the API server enables the % feature gate%'
                  or d.value->>'description' ilike '%honored by servers that enable the % feature%'
                  or d.value->>'description' ilike '%requires enabling % feature gate%'
                  or d.value->>'description' ilike '%honored by clusters that enables the % feature%'
                  or d.value->>'description' ilike '%only if the % feature gate is enabled%'
                  ) THEN true
            ELSE false
              END AS feature_gated,
            d.value->>'format' AS format,
            d.value->>'x-kubernetes-patch-merge-key' AS merge_key,
            d.value->>'x-kubernetes-patch-strategy' AS patch_strategy,
            api_schema.bucket,
            api_schema.job,
            d.value
       FROM (api_schema
             JOIN LATERAL jsonb_each(api_schema.properties) d(key, value) ON (true));
 #+END_SRC

 #+RESULTS: api_schema_field view
 #+begin_src sql-mode
 CREATE VIEW
 #+end_src

** 300: grkrv

*** 310: Audit Events By GVKRV(Group, Version, Kind, Resource(s),Verb)
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/310_view_audit_event_by_gvkrv.up.sql
    :END:
  
   This is a slim view, and will need to be updated to contain all useful info if/when we phase out operationID across reports.
     #+NAME: events by gvkrv
     #+BEGIN_SRC sql-mode :results silent
       CREATE OR REPLACE VIEW "public"."audit_events_by_gvkrv" AS
         SELECT
           CASE
           WHEN ((a.data -> 'objectRef' ->> 'apiGroup') IS NULL) THEN ''
           ELSE (a.data -> 'objectRef' ->> 'apiGroup')
                 END as api_group,
           (a.data -> 'objectRef' ->>'apiVersion') as api_version,
           (a.data -> 'requestObject'->>'kind') as kind,
           a.param_schema as body_schema,
           (a.data -> 'objectRef'->>'resource') as resource,
             (a.data -> 'objectRef'->>'subresource') as sub_resource,
           (a.data->>'verb') as event_verb,
           operation_id,
           audit_id,
           split_part(a.useragent, '--', 2) as test,
           split_part(a.useragent, '--', 1) as useragent,
           (a.data -> 'requestObject') as request_object,
           bucket,
           job
           FROM audit_event as a
          where data->'requestObject' is not null;
     #+END_SRC
  
** 400: Podspec Field Views
   :PROPERTIES:
   :header-args:sql-mode+: :results silent
   :END:
*** 400: kind_field_path_recursion
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/400_view_kind_field_recursion.up.sql
    :END:
 #+NAME: Recursive kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace recursive view kind_field_path_recursion(
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
   ) AS
    SELECT DISTINCT
    sf.field_schema AS kind,
    sf.field_name AS field_path, -- this becomes a path
    sf.field_kind AS field_kind,
    sf.field_type AS field_type,
    sf.field_schema AS sub_kind, -- this is the kind at this level
    sf.release AS release,
    sf.deprecated AS deprecated, 
    sf.feature_gated AS feature_gated,
    sf.required AS required,
    sf.bucket as bucket,
    sf.job as job
    from api_schema_field sf
    UNION
    SELECT
     kfpr.kind AS kind,
     ( kfpr.field_path || '.' || f.field_name ) AS field_path,
     f.field_kind AS field_kind,
     f.field_type AS field_type,
     CASE
     WHEN f.field_kind = 'string' OR f.field_kind = 'integer' THEN f.field_schema
     ELSE f.field_kind
      END as sub_kind,
     f.release AS release,
     f.deprecated AS deprecated,
     f.feature_gated AS feature_gated,
     f.required AS required,
     kfpr.bucket,
     kfpr.job
     FROM api_schema_field f
     INNER JOIN kind_field_path_recursion kfpr ON
     f.field_schema = kfpr.field_kind
     AND f.field_kind not like 'io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.%.JSONSchemaProps';
   ;
 #+END_SRC
*** 410: kind_field_path_material
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/410_view_kind_field_path_material.up.sql
    :END:
 #+NAME: kind_field_path material
 #+BEGIN_SRC sql-mode
    create materialized view kind_field_path_material AS
    select
      kind,
      field_path AS field_path,
      field_kind AS field_kind,
      field_type,
      sub_kind,
      release,
      deprecated,
      gated,
      required,
      bucket,
      job
     from kind_field_path_recursion;
   -- drop materialized view kind_field_path_material cascade;
 #+END_SRC
**** kind_field_path_material indexes
 #+NAME: kind_field_path_material indexs
 #+BEGIN_SRC sql-mode
 CREATE INDEX kfpm_kind_idx       ON kind_field_path_material (kind);
 CREATE INDEX kfpm_field_path_idx ON kind_field_path_material (field_path);
 CREATE INDEX kfpm_field_type_idx ON kind_field_path_material (field_type);
 CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material (sub_kind);
 -- GIST requires ltree
 -- CREATE INDEX kfpm_kind_idx       ON kind_field_path_material USING GIST (kind);
 -- CREATE INDEX kfpm_field_path_idx ON kind_field_path_material USING GIST (field_path);
 -- CREATE INDEX kfpm_field_type_idx ON kind_field_type_material USING GIST (field_type);
 -- CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material USING GIST (sub_kind);
 #+END_SRC

*** 420: kind_field_path view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/420_view_kind_field_path.up.sql
    :END:
 #+NAME: kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace view kind_field_path AS
   select
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
    from kind_field_path_material where field_kind not like 'io%';
 #+END_SRC

*** 430: PodSpec Materialized View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/430_podspec_field_coverage_material.up.sql
    :END:
    
    #+NAME: view podspec_field_coverage_material
    #+BEGIN_SRC sql-mode :results silent
      CREATE MATERIALIZED VIEW "public"."podspec_field_coverage_material" AS 
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec'::text) event_field(event_field)
       WHERE kind = 'Pod'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}')) -- api_version doesn't contain alpha or beta;
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = 'PodTemplate'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec' -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec' -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = ANY('{DaemonSet, Deployment, ReplicationController, StatefulSet, Job,ReplicaSet}')
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field; 
   #+END_SRC
  
   #+BEGIN_SRC sql-mode
 select distinct bucket, job from podspec_field_coverage_material;
   #+END_SRC

*** 440: PodSpec Field Coverage View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/440_view_podspec_field_coverage.up.sql
    :END:
 #+NAME: view podspec_field_coverage
 #+BEGIN_SRC sql-mode
 create view podspec_field_coverage as select * from podspec_field_coverage_material;
 #+END_SRC
 
*** 450: PodSpec Field Summary View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/450_view_podspec_field_summary.up.sql
    :END:
 #+NAME: view podspec_field_summary
 #+BEGIN_SRC sql-mode
   create view podspec_field_summary as
     select distinct field_name as podspec_field,
                     0 as other_hits,
                     0 as e2e_hits,
                     0 as conf_hits,
                     bucket,
                     job
       from api_schema_field
      where field_schema like '%PodSpec%'
      UNION
     select
       podspec_field,
       sum(hits) as other_hits,
       0 as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent not like 'e2e.test%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       sum(hits) as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test not like '%Conformance%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       0 as e2e_hits,
       sum(hits) as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test like '%Conformance%'
      group by podspec_field, bucket, job;
 #+END_SRC
*** 460: PodSpec Field mid Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/460_view_podspec_field_mid_report.up.sql
    :END:
  #+NAME: podspec_field_mid_report
  #+BEGIN_SRC sql-mode :results silent
    create or replace view podspec_field_mid_report as
    select distinct podspec_field,
          sum(other_hits) as other_hits,
          sum(e2e_hits) as e2e_hits,
          sum(conf_hits) as conf_hits,
          kfp.release,
          kfp.deprecated,
          kfp.gated,
          kfp.required,
          kfp.field_kind,
          kfp.field_type,
          pfs.bucket, 
          pfs.job
    from podspec_field_summary pfs, kind_field_path_recursion kfp
    where 
      kfp.kind = 'io.k8s.api.core.v1.PodSpec'
      and pfs.podspec_field = kfp.field_path
    group by podspec_field, kfp.release, kfp.deprecated, kfp.gated, kfp.required, kfp.field_kind, kfp.field_type, pfs.bucket, pfs.job
    order by conf_hits, e2e_hits, other_hits;
  #+END_SRC

*** 470: PodSpec Field Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/470_view_podspec_field_report.up.sql
    :END:
 #+NAME: podspec_field_hits
 #+BEGIN_SRC sql-mode
   create or replace view podspec_field_report as
   select distinct podspec_field,
         sum(other_hits) as other_hits,
         sum(e2e_hits) as e2e_hits,
         sum(conf_hits) as conf_hits,
         release,
         deprecated,
         gated,
         required,
         field_kind,
         field_type,
         bucket,
         job
   from podspec_field_mid_report
   group by podspec_field, release, deprecated, gated, required, field_kind, field_type, bucket, job
   order by conf_hits, e2e_hits, other_hits;
 #+END_SRC
 
 #+BEGIN_SRC sql-mode :results replace drawer
   select
     podspec_field, e2e_hits, pfr.job, bjs.job_timestamp
     from podspec_field_report pfr
     JOIN bucket_job_swagger bjs on(bjs.bucket = pfr.bucket AND bjs.job = pfr.job) 
    order by podspec_field;
 #+END_SRC

 #+RESULTS:
 :results:
          podspec_field         | e2e_hits |         job         |    job_timestamp    
 -------------------------------+----------+---------------------+---------------------
  activeDeadlineSeconds         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  activeDeadlineSeconds         |        0 | live                | 2019-12-04 20:14:50
  affinity                      |        0 | live                | 2019-12-04 20:14:50
  affinity                      |     2264 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |        0 | live                | 2019-12-04 20:14:50
  containers                    |        0 | live                | 2019-12-04 20:14:50
  containers                    |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsConfig                     |        0 | live                | 2019-12-04 20:14:50
  dnsConfig                     |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |        0 | live                | 2019-12-04 20:14:50
  enableServiceLinks            |    26592 | 1202311785298792448 | 2019-12-04 20:14:50
  enableServiceLinks            |        0 | live                | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | live                | 2019-12-04 20:14:50
  hostAliases                   |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  hostAliases                   |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |        0 | live                | 2019-12-04 20:14:50
  hostNetwork                   |     6296 | 1202311785298792448 | 2019-12-04 20:14:50
  hostNetwork                   |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | live                | 2019-12-04 20:14:50
  initContainers                |     3944 | 1202311785298792448 | 2019-12-04 20:14:50
  initContainers                |        0 | live                | 2019-12-04 20:14:50
  nodeName                      |    18476 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeName                      |        0 | live                | 2019-12-04 20:14:50
  nodeSelector                  |     2252 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeSelector                  |        0 | live                | 2019-12-04 20:14:50
  overhead                      |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  overhead                      |        0 | live                | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | live                | 2019-12-04 20:14:50
  priority                      |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  priority                      |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |      128 | 1202311785298792448 | 2019-12-04 20:14:50
  readinessGates                |        0 | live                | 2019-12-04 20:14:50
  readinessGates                |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |        0 | live                | 2019-12-04 20:14:50
  securityContext               |        0 | live                | 2019-12-04 20:14:50
  securityContext               |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |        0 | live                | 2019-12-04 20:14:50
  serviceAccountName            |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccountName            |        0 | live                | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |        0 | live                | 2019-12-04 20:14:50
  tolerations                   |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  tolerations                   |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |    27044 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |        0 | live                | 2019-12-04 20:14:50
 (68 rows)

 :end:

*** 480: materialized kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/480_kind_field_path_coverage_material.up.sql
    :END:
    This is the base view we use to traverse the paths later.  It grabs all relevant fields from our kind_field_path_recursion and joins it to our audit_events based on where the request_object of the event includes the relevant fieldpath.
   
    #+NAME: kind_field_path_coverage_material_improved
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."kind_field_path_coverage_material" AS
      SELECT
        kfpr.bucket,
        kfpr.job,
        kfpr.kind,
        kfpr.field_path,
        kfpr.field_kind,
        kfpr.sub_kind,
        (array_length(string_to_array(kfpr.field_path, '.'),1) - 1) as distance,
        ae.audit_id as audit_event_id,
        ae.useragent as useragent,
        ae.operation_id
        FROM kind_field_path_recursion kfpr
            LEFT JOIN LATERAL (select * from audit_event WHERE param_schema = kfpr.kind AND jsonb_path_exists(request_object, ('$.'||kfpr.field_path)::jsonpath)) ae ON true
        GROUP BY kfpr.kind, kfpr.field_path, kfpr.field_kind, kfpr.bucket, kfpr.job, kfpr.sub_kind, ae.audit_id, ae.useragent, ae.operation_id; 
    #+END_SRC
    #+begin_src sql-mode
     refresh materialized view kind_field_path_coverage_material; 
    #+end_src
   
*** 485: kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/485_kind_field_path_coverage.up.sql
    :END:
    A view into our material,  so hasura can track it.
    #+NAME: kind_field_path_coverage
    #+BEGIN_SRC sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material;
    #+END_SRC
*** 490: materialized full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/490_full_podspec_field_coverage_material.up.sql
    :END:
    We want a subset of this grand field_coverage view, looking only for fields that come from Podspec.    
    This is going to look across all our buckets and jobs, so it will take a bit of time to materialize.
   
    We are only looking at the stable, core kinds or the GA kinds.
   
    #+NAME: full_podspec_field_coverage_material
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."full_podspec_field_coverage_material" AS
        WITH podspec_kinds AS (
              SELECT DISTINCT kind, field_path
                FROM kind_field_path_coverage
                 WHERE field_kind = 'io.k8s.api.core.v1.PodSpec'
                 AND kind not like '%alpha%'
                 AND kind not like '%beta%'
                 AND operation_id is not null
        )
        SELECT DISTINCT
          trim(leading 'io.k8s.api.' from c.kind) as kind,
          trim(leading 'io.k8s.api.' from c.sub_kind) as  sub_kind,
          c.field_path,
          distance,
          count(*) FILTER(WHERE c.useragent like 'e2e.test%') as test_hits,
          count(*) FILTER(WHERE c.useragent like '%[Conformance]%') as conf_hits,
          c.field_kind,
          c.job,
          c.bucket
          FROM kind_field_path_coverage c
            INNER JOIN podspec_kinds pk ON (c.kind = pk.kind AND c.field_path like  pk.field_path || '%')
            and sub_kind not like '%VolumeSource'
            GROUP BY c.sub_kind, c.kind, c.field_path, c.field_kind, c.distance, pk.field_path, c.job, c.bucket
            ORDER BY field_path;
    #+END_SRC

    #+begin_src sql-mode
    drop materialized view full_podspec_field_coverage_material cascade;

    #+end_src
*** 495: full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/495_full_podspec_field_coverage.up.sql
    :END:
    And we can create a view from this
    #+NAME: full_podspec_field_coverage
    #+BEGIN_SRC sql-mode
     CREATE OR REPLACE VIEW "public"."full_podspec_field_coverage" AS
      select * from full_podspec_field_coverage_material;
    #+END_SRC
   
   
    When using the view, you will want ot make sure to limit it by a job, otherwise you'll get massive results.
   
    for example


     #+begin_src sql-mode :results replace code :wrap EXAMPLE
     \d+ full_podspec_field_coverage;
     #+end_src

     #+RESULTS:
     #+begin_EXAMPLE
                        View "public.full_podspec_field_coverage"
        Column   |  Type   | Collation | Nullable | Default | Storage  | Description 
     ------------+---------+-----------+----------+---------+----------+-------------
      kind       | text    |           |          |         | extended | 
      sub_kind   | text    |           |          |         | extended | 
      field_path | text    |           |          |         | extended | 
      distance   | integer |           |          |         | plain    | 
      test_hits  | bigint  |           |          |         | plain    | 
      conf_hits  | bigint  |           |          |         | plain    | 
      field_kind | text    |           |          |         | extended | 
      job        | text    |           |          |         | extended | 
      bucket     | text    |           |          |         | extended | 
     View definition:
      SELECT full_podspec_field_coverage_material.kind,
         full_podspec_field_coverage_material.sub_kind,
         full_podspec_field_coverage_material.field_path,
         full_podspec_field_coverage_material.distance,
         full_podspec_field_coverage_material.test_hits,
         full_podspec_field_coverage_material.conf_hits,
         full_podspec_field_coverage_material.field_kind,
         full_podspec_field_coverage_material.job,
         full_podspec_field_coverage_material.bucket
        FROM full_podspec_field_coverage_material;

     #+end_EXAMPLE
    #+name: kind_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view kind_field_path_coverage_material; 
    #+end_src
    #+name: full_podspec_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."full_podspec_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view full_podspec_field_path_coverage_material; 
    #+end_src

   
** for aaron                                                         :export:
    #+begin_src sql-mode :results replace :tangle no :eval never-export :exports both :file results.txt
    select kind, sub_kind, field_path, test_hits, distance from full_podspec_field_coverage where job != 'live';
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
    [[file:results.txt]]
    #+end_src

** ASKS
*** kindnet-image pull.... requires internet makes sad
*** remove latest tag on auditlogger, replace with date
*** auditlogger depend on hasura (similar to hasura -> pg)
*** :eval ask for kind cluster delete
    or move to code block eval :never or move to own block
*** put the kind image pull stuff in footnotes
 with not to look at [[#footnotes]]
*** fix namespace stuf... maybe use default instead of kube-system
*** kindnetd old
   #+BEGIN_SRC tmate :eval never-export
     kind load image-archive            kindnetd:aa67fec7d7ef7.docker-image \
       || docker pull docker.io/kindest/kindnetd:aa67fec7d7ef7 \
       && docker save docker.io/kindest/kindnetd:aa67fec7d7ef7 -o kindnetd:aa67fec7d7ef7.docker-image
   #+END_SRC
*** kind load via registry
   #+BEGIN_SRC tmate :eval never-export
     # Seems a bit slow... loads from image-archives are much faster
     # kind load docker-image --name=kind-$USER raiinbow/hasura:2019-12-03-16-31 
     # kind load docker-image --name=kind-$USER raiinbow/postgres:2019-12-03-14-19
     # kind load docker-image --name=kind-$USER raiinbow/auditlogger:latest
   #+END_SRC

