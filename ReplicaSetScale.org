# -*- ii: apisnoop; -*-
#+TITLE: Mock Ticket Template
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
#+PROPERTY: header-args:sql-mode :product postgres

* TODO Progress [2/5]                                                :export:
- [X] APISnoop org-flow : [[https://github.com/cncf/apisnoop/blob/master/tickets/k8s/][MyEndpoint.org]]
- [X] test approval issue : [[https://github.com/kubernetes/kubernetes/issues/][kubernetes/kubernetes#]]
- [ ] test pr : kuberenetes/kubernetes#
- [ ] two weeks soak start date : testgrid-link
- [ ] two weeks soak end date :
- [ ] test promotion pr : kubernetes/kubernetes#?
* Identifying an untested feature Using APISnoop                     :export:

According to this APIsnoop query, there are still some remaining RESOURCENAME endpoints which are untested.

with this query you can filter untested endpoints by their category and eligiblity for conformance.
e.g below shows a query to find all conformance eligible untested,stable,core endpoints

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      endpoint,
      -- k8s_action,
      -- path,
      -- description,
      kind
      FROM testing.untested_stable_endpoint
      where eligible is true
        and endpoint like '%ReplicaSet%'
      --and category = 'core'
      order by kind, endpoint desc
      limit 25;
  #+end_src

 #+RESULTS: untested_stable_core_endpoints
 #+begin_SRC example
                   endpoint                  |    kind
 --------------------------------------------+------------
  replaceAppsV1NamespacedReplicaSetStatus    | ReplicaSet
  readAppsV1NamespacedReplicaSetStatus       | ReplicaSet
  patchAppsV1NamespacedReplicaSetStatus      | ReplicaSet
  patchAppsV1NamespacedReplicaSet            | ReplicaSet
  listAppsV1ReplicaSetForAllNamespaces       | ReplicaSet
  deleteAppsV1CollectionNamespacedReplicaSet | ReplicaSet
  replaceAppsV1NamespacedReplicaSetScale     | Scale
  readAppsV1NamespacedReplicaSetScale        | Scale
  patchAppsV1NamespacedReplicaSetScale       | Scale
 (9 rows)

 #+end_SRC

* API Reference and feature documentation                            :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [[https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/RESOURCENAME.go][client-go - RESOURCENAME]]

* The mock test                                                      :export:
** Test outline
1. Create a RESOURCENAME with a static label

2. Patch the RESOURCENAME with a new label and updated data

3. Get the RESOURCENAME to ensure it's patched

4. List all RESOURCENAMEs in all Namespaces with a static label
   find the RESOURCENAME
   ensure that the RESOURCENAME is found and is patched

5. Delete Namespaced RESOURCENAME via a Collection with a LabelSelector

** Test outline in Kubectl

*** 1. Create a ReplicaSet yaml file, namespace and Deployment


#+begin_src yaml :tangle ReplicaSet_test.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3


#+end_src
- Tangle to create the .yaml file - `,bt`


#+begin_src yaml :tangle replicaset_test.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend2
  labels:
    app: guestbook
    tier: frontend2
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend2
  template:
    metadata:
      labels:
        tier: frontend2
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3


#+end_src
- See if the yaml file was created
#+begin_src shell :results raw
  pwd
# ls -al /home/riaan/Project/ticket-writing |grep .yaml

 ls -al /home/ii/ticket-writing | grep yaml
#+end_src

#+RESULTS:
#+begin_example
/home/ii/ticket-writing
-rw-r--r--  1 ii ii    512 Jan 21 11:00 PatchReplicaSet_test.yaml
-rw-r--r--  1 ii ii    444 Jan 21 11:00 replicaset_test.yaml
-rw-r--r--  1 ii ii    456 Jan 21 11:00 ReplicaSet_test.yaml
#+end_example







- Create a Namespace
#+begin_src shell :results raw
kubectl create namespace app-replicaset-tests

#+end_src

#+RESULTS:
#+begin_example
namespace/app-replicaset-tests created
#+end_example




- Creating a Replicaset

#+begin_src shell :results raw
kubectl apply -f ReplicaSet_test.yaml --namespace=app-replicaset-tests
kubectl apply -f replicaset_test.yaml --namespace=app-replicaset-tests
#+end_src

#+RESULTS:
#+begin_example
replicaset.apps/frontend created
replicaset.apps/frontend2 created
#+end_example




- Finding the ReplicaSet
#+begin_src shell :results raw
kubectl get rs -A | grep frontend
#+end_src

#+RESULTS:
#+begin_example
app-replicaset-tests   frontend                                            3         3         3       19s
app-replicaset-tests   frontend2                                           3         3         3       19s
#+end_example



- Scale the replicaset
#+begin_src shell :results raw
kubectl scale rs frontend -n app-replicaset-tests --replicas=5
#+end_src

#+RESULTS:
#+begin_example
replicaset.apps/frontend scaled
#+end_example




- look for scaled replicas
#+begin_src shell :results raw
  kubectl get rs -A | grep frontend
#+end_src

#+RESULTS:
#+begin_example
app-replicaset-tests   frontend                                            5         5         5       57s
app-replicaset-tests   frontend2                                           3         3         3       57s
#+end_example



- Scale the replicaset again - Add no new endpoint
#+begin_src shell :results raw
#kubectl scale rs frontend -n app-replicaset-tests --replicas=6
#+end_src






- Create a PATCH .yaml file

#+begin_src yaml :tangle PatchReplicaSet_test.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 7
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3


#+end_src


- Patch the ReplicaSet

#+begin_src shell :results raw
kubectl patch rs frontend -n app-replicaset-tests --patch "$(cat PatchReplicaSet_test.yaml)"


#+end_src

#+RESULTS:
#+begin_example
replicaset.apps/frontend patched
#+end_example




- Look for the Patched ReplicaSet

#+begin_src shell :results raw
  kubectl get rs -A | grep frontend

#+end_src

#+RESULTS:
#+begin_example
app-replicaset-tests   frontend                                            7         7         7       2m3s
app-replicaset-tests   frontend2                                           3         3         3       2m3s
#+end_example


Get the SCALE for RS
#+begin_src shell :results raw
kubectl get --raw /apis/apps/v1/namespaces/app-replicaset-tests/replicasets/frontend/scale


#+end_src

#+RESULTS:
#+begin_example
{"kind":"Scale","apiVersion":"autoscaling/v1","metadata":{"name":"frontend","namespace":"app-replicaset-tests","uid":"d577d314-5bd3-4561-8a27-2b184999f12b","resourceVersion":"73557","creationTimestamp":"2021-01-20T23:10:06Z"},"spec":{"replicas":7},"status":{"replicas":7,"selector":"tier=frontend"}}
#+end_example








#+begin_src shell :results raw
kubectl delete replicasets -n app-replicaset-tests --all

#+end_src



- Clean-up

#+begin_src shell :results raw
kubectl delete rs frontend
kubectl delete namespaces/app-replicaset-tests
#+end_src

#+RESULTS:
#+begin_example
namespace "app-replicaset-tests" deleted
#+end_example



#+begin_src shell :results raw
kubectl get rs -A | grep frontend
kubectl get namespace -A | grep replicaset
#+end_src

#+RESULTS:
#+begin_example
#+end_example





- Delete audit events to check for success

- Count all audit events
#+begin_src sql-mode
select count(*) from testing.audit_event;
#+end_src

#+RESULTS:
#+begin_SRC example
  count
---------
 2270944
(1 row)

#+end_SRC


- Delete all audit events
#+begin_src sql-mode
delete from testing.audit_event;
#+end_src

#+RESULTS:
#+begin_SRC example
DELETE 2271649
#+end_SRC





- Test to see is new endpoint was hit by the test
#+begin_src sql-mode :eval never-export :exports both :session none
  select distinct  endpoint, useragent
                   -- to_char(to_timestamp(release_date::bigint), ' HH:MI') as time
  from testing.audit_event
  where endpoint ilike '%ReplicaSet%'
    -- and release_date::BIGINT > round(((EXTRACT(EPOCH FROM NOW()))::numeric)*1000,0) - 60000
  and useragent like 'kubectl%'
  order by endpoint
  limit 100;

#+end_src

#+RESULTS:
#+begin_SRC example
               endpoint               |                    useragent
--------------------------------------+--------------------------------------------------
 createAppsV1NamespacedReplicaSet     | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 deleteAppsV1NamespacedReplicaSet     | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 listAppsV1NamespacedReplicaSet       | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 listAppsV1ReplicaSetForAllNamespaces | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 patchAppsV1NamespacedReplicaSet      | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 patchAppsV1NamespacedReplicaSetScale | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 readAppsV1NamespacedReplicaSet       | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
 readAppsV1NamespacedReplicaSetScale  | kubectl/v1.19.4 (linux/amd64) kubernetes/d360454
(8 rows)

#+end_SRC



- Untested endpoint hit by Kubectl commands
listAppsV1ReplicaSetForAllNamespaces
patchAppsV1NamespacedReplicaSet
patchAppsV1NamespacedReplicaSetScale
readAppsV1NamespacedReplicaSetScale


** Test the functionality in Go
   #+NAME: Mock Test In Go
   #+begin_src go
     package main

     import (
       // "encoding/json"
       "fmt"
       "context"
       "flag"
       "os"
       v1 "k8s.io/api/core/v1"
       // "k8s.io/client-go/dynamic"
       // "k8s.io/apimachinery/pkg/runtime/schema"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       // "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
           fmt.Println(err, "Could not build config from flags")
           return
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)
       // DynamicClientSet, _ := dynamic.NewForConfig(config)
       // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

       // TEST BEGINS HERE

       testPodName := "test-pod"
       testPodImage := "nginx"
       testNamespaceName := "default"

       fmt.Println("creating a Pod")
       testPod := v1.Pod{
         ObjectMeta: metav1.ObjectMeta{
           Name: testPodName,
           Labels: map[string]string{"test-pod-static": "true"},
         },
         Spec: v1.PodSpec{
           Containers: []v1.Container{{
             Name: testPodName,
             Image: testPodImage,
           }},
         },
       }
       _, err = ClientSet.CoreV1().Pods(testNamespaceName).Create(context.TODO(), &testPod, metav1.CreateOptions{})
       if err != nil {
           fmt.Println(err, "failed to create Pod")
           return
       }

       fmt.Println("listing Pods")
       pods, err := ClientSet.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{LabelSelector: "test-pod-static=true"})
       if err != nil {
           fmt.Println(err, "failed to list Pods")
           return
       }
       podCount := len(pods.Items)
       if podCount == 0 {
           fmt.Println("there are no Pods found")
           return
       }
       fmt.Println(podCount, "Pod(s) found")

       fmt.Println("deleting Pod")
       err = ClientSet.CoreV1().Pods(testNamespaceName).Delete(context.TODO(), testPodName, metav1.DeleteOptions{})
       if err != nil {
           fmt.Println(err, "failed to delete the Pod")
           return
       }

       // TEST ENDS HERE

       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_example
   creating a Pod
   listing Pods
   1 Pod(s) found
   deleting Pod
   [status] complete
   #+end_example





** Test the functionality in Go
   #+NAME: Mock Test In Go
   #+begin_src go
     package main

     import (
       // "encoding/json"
       "fmt"
       "context"
       "flag"
       "os"
       v1 "k8s.io/api/core/v1"
       // "k8s.io/client-go/dynamic"
       // "k8s.io/apimachinery/pkg/runtime/schema"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       // "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
           fmt.Println(err, "Could not build config from flags")
           return
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)
       // DynamicClientSet, _ := dynamic.NewForConfig(config)
       // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

       // TEST BEGINS HERE

       testPodName := "test-pod"
       testPodImage := "nginx"
       testNamespaceName := "default"

       fmt.Println("creating a Pod")
       testPod := v1.Pod{
         ObjectMeta: metav1.ObjectMeta{
           Name: testPodName,
           Labels: map[string]string{"test-pod-static": "true"},
         },
         Spec: v1.PodSpec{
           Containers: []v1.Container{{
             Name: testPodName,
             Image: testPodImage,
           }},
         },
       }
       _, err = ClientSet.CoreV1().Pods(testNamespaceName).Create(context.TODO(), &testPod, metav1.CreateOptions{})
       if err != nil {
           fmt.Println(err, "failed to create Pod")
           return
       }

       fmt.Println("listing Pods")
       pods, err := ClientSet.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{LabelSelector: "test-pod-static=true"})
       if err != nil {
           fmt.Println(err, "failed to list Pods")
           return
       }
       podCount := len(pods.Items)
       if podCount == 0 {
           fmt.Println("there are no Pods found")
           return
       }
       fmt.Println(podCount, "Pod(s) found")

       fmt.Println("deleting Pod")
       err = ClientSet.CoreV1().Pods(testNamespaceName).Delete(context.TODO(), testPodName, metav1.DeleteOptions{})
       if err != nil {
           fmt.Println(err, "failed to delete the Pod")
           return
       }

       // TEST ENDS HERE

       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_example
   creating a Pod
   listing Pods
   1 Pod(s) found
   deleting Pod
   [status] complete
   #+end_example

* Verifying increase in coverage with APISnoop                       :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent
      from testing.audit_event
      where useragent like 'live%';
  #+end_src

  #+RESULTS:
  :  useragent
  : -----------
  : (0 rows)
  :

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from testing.endpoint_hit_by_new_test;
#+end_src

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from testing.projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             438 |          183 |          183 |                0
  (1 row)

  #+end_SRC

* Convert to Ginkgo Test
** Ginkgo Test
  :PROPERTIES:
  :ID:       gt001z4ch1sc00l
  :END:
* Final notes                                                        :export:
If a test with these calls gets merged, **test coverage will go up by N points**

This test is also created with the goal of conformance promotion.

-----
/sig testing

/sig architecture

/area conformance


* scratch
#+BEGIN_SRC
CREATE OR REPLACE VIEW "public"."untested_stable_endpoints" AS
  SELECT
    ec.*,
    ao.description,
    ao.http_method
    FROM endpoint_coverage ec
           JOIN
           api_operation_material ao ON (ec.bucket = ao.bucket AND ec.job = ao.job AND ec.operation_id = ao.operation_id)
   WHERE ec.level = 'stable'
     AND tested is false
     AND ao.deprecated IS false
     AND ec.job != 'live'
   ORDER BY hit desc
            ;
#+END_SRC
