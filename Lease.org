# -*- ii: apisnoop; -*-
#+TITLE: Mock Ticket Template
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
#+PROPERTY: header-args:sql-mode :product postgres

* TODO Progress [2/5]                                                :export:
- [X] APISnoop org-flow : [[https://github.com/cncf/apisnoop/blob/master/tickets/k8s/][MyEndpoint.org]]
- [X] test approval issue : [[https://github.com/kubernetes/kubernetes/issues/][kubernetes/kubernetes#]]
- [ ] test pr : kuberenetes/kubernetes#
- [ ] two weeks soak start date : testgrid-link
- [ ] two weeks soak end date :
- [ ] test promotion pr : kubernetes/kubernetes#?
* Identifying an untested feature Using APISnoop                     :export:

According to this APIsnoop query, there are still some remaining RESOURCENAME endpoints which are untested.

with this query you can filter untested endpoints by their category and eligiblity for conformance.
e.g below shows a query to find all conformance eligible untested,stable,core endpoints

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      endpoint,
      -- k8s_action,
      -- path,
      -- description,
      kind
      FROM testing.untested_stable_endpoint
      where eligible is true
      and category = 'core'
      order by kind, endpoint desc
      limit 25;
  #+end_src

 #+RESULTS: untested_stable_core_endpoints
 #+begin_example
                   endpoint                  |       kind
 --------------------------------------------+------------------
  createCoreV1NamespacedPodBinding           | Binding
  createCoreV1NamespacedBinding              | Binding
  replaceCoreV1NamespacedEvent               | Event
  readCoreV1NamespacedEvent                  | Event
  patchCoreV1NamespacedEvent                 | Event
  listCoreV1NamespacedEvent                  | Event
  listCoreV1EventForAllNamespaces            | Event
  deleteCoreV1NamespacedEvent                | Event
  deleteCoreV1CollectionNamespacedEvent      | Event
  createCoreV1NamespacedEvent                | Event
  patchCoreV1NamespacedLimitRange            | LimitRange
  listCoreV1LimitRangeForAllNamespaces       | LimitRange
  deleteCoreV1CollectionNamespacedLimitRange | LimitRange
  replaceCoreV1NamespaceStatus               | Namespace
  replaceCoreV1NamespaceFinalize             | Namespace
  readCoreV1NamespaceStatus                  | Namespace
  patchCoreV1NamespaceStatus                 | Namespace
  replaceCoreV1NodeStatus                    | Node
  readCoreV1NodeStatus                       | Node
  deleteCoreV1CollectionNode                 | Node
  connectCoreV1PutNodeProxyWithPath          | NodeProxyOptions
  connectCoreV1PutNodeProxy                  | NodeProxyOptions
  connectCoreV1PostNodeProxyWithPath         | NodeProxyOptions
  connectCoreV1PostNodeProxy                 | NodeProxyOptions
  connectCoreV1PatchNodeProxyWithPath        | NodeProxyOptions
 (25 rows)

 #+end_example

* API Reference and feature documentation                            :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [[https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/RESOURCENAME.go][client-go - RESOURCENAME]]

* The mock test                                                      :export:
** Test outline
1. Create a RESOURCENAME with a static label

2. Patch the RESOURCENAME with a new label and updated data

3. Get the RESOURCENAME to ensure it's patched

4. List all RESOURCENAMEs in all Namespaces with a static label
   find the RESOURCENAME
   ensure that the RESOURCENAME is found and is patched

5. Delete Namespaced RESOURCENAME via a Collection with a LabelSelector

** Test the functionality in Go
   #+NAME: Mock Test In Go
   #+begin_src go
     package main

     import (
       // "encoding/json"
       "fmt"
       "context"
       "flag"
       "os"
       v1 "k8s.io/api/core/v1"
       // "k8s.io/client-go/dynamic"
       // "k8s.io/apimachinery/pkg/runtime/schema"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       // "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
           fmt.Println(err, "Could not build config from flags")
           return
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)
       // DynamicClientSet, _ := dynamic.NewForConfig(config)
       // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

       // TEST BEGINS HERE

       testPodName := "test-pod"
       testPodImage := "nginx"
       testNamespaceName := "default"

       fmt.Println("creating a Pod")
       testPod := v1.Pod{
         ObjectMeta: metav1.ObjectMeta{
           Name: testPodName,
           Labels: map[string]string{"test-pod-static": "true"},
         },
         Spec: v1.PodSpec{
           Containers: []v1.Container{{
             Name: testPodName,
             Image: testPodImage,
           }},
         },
       }
       _, err = ClientSet.CoreV1().Pods(testNamespaceName).Create(context.TODO(), &testPod, metav1.CreateOptions{})
       if err != nil {
           fmt.Println(err, "failed to create Pod")
           return
       }

       fmt.Println("listing Pods")
       pods, err := ClientSet.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{LabelSelector: "test-pod-static=true"})
       if err != nil {
           fmt.Println(err, "failed to list Pods")
           return
       }
       podCount := len(pods.Items)
       if podCount == 0 {
           fmt.Println("there are no Pods found")
           return
       }
       fmt.Println(podCount, "Pod(s) found")

       fmt.Println("deleting Pod")
       err = ClientSet.CoreV1().Pods(testNamespaceName).Delete(context.TODO(), testPodName, metav1.DeleteOptions{})
       if err != nil {
           fmt.Println(err, "failed to delete the Pod")
           return
       }

       // TEST ENDS HERE

       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_example
   creating a Pod
   listing Pods
   1 Pod(s) found
   deleting Pod
   [status] complete
   #+end_example

* Verifying increase in coverage with APISnoop                       :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent
      from testing.audit_event
      where useragent like 'live%';
  #+end_src

  #+RESULTS:
  :  useragent
  : -----------
  : (0 rows)
  :

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from testing.endpoint_hit_by_new_test;
#+end_src

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from testing.projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             438 |          183 |          183 |                0
  (1 row)

  #+end_SRC

* Convert to Ginkgo Test
** Ginkgo Test
  :PROPERTIES:
  :ID:       gt001z4ch1sc00l
  :END:
* Final notes                                                        :export:
If a test with these calls gets merged, **test coverage will go up by N points**

This test is also created with the goal of conformance promotion.

-----
/sig testing

/sig architecture

/area conformance


* scratch
#+BEGIN_SRC
CREATE OR REPLACE VIEW "public"."untested_stable_endpoints" AS
  SELECT
    ec.*,
    ao.description,
    ao.http_method
    FROM endpoint_coverage ec
           JOIN
           api_operation_material ao ON (ec.bucket = ao.bucket AND ec.job = ao.job AND ec.operation_id = ao.operation_id)
   WHERE ec.level = 'stable'
     AND tested is false
     AND ao.deprecated IS false
     AND ec.job != 'live'
   ORDER BY hit desc
            ;
#+END_SRC
